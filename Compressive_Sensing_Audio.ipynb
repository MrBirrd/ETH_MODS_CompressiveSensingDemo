{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBjawd9U-b5E"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib.pyplot import plot, show, figure, title\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from scipy.fftpack import dct, idct\n",
        "from scipy.io import wavfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy7Tp9hG5h6E"
      },
      "source": [
        "# This notebook should demonstrate the power of random projections applied to music, since we can observe it directly by listening to the results. All the theory can be found in the lecture notes and when needed, I linked additional resources.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DzFeShjibEc"
      },
      "source": [
        "# We start by defining our signal. We use an addition of simple sine waves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Q1DDd-CiaB"
      },
      "source": [
        "General settings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdh6ahhB7nWf"
      },
      "outputs": [],
      "source": [
        "# Fs defines the sampling rate of one would normally use.\n",
        "# 44.1kHz is a usual sample rate for consumer audio files like mp3\n",
        "Fs = 44100  \n",
        "\n",
        "# the duration of the sample in seconds\n",
        "# note that it should not really exceed more than one second\n",
        "# since this means an input vector of 44100 values and can fill\n",
        "# your RAM very quick.\n",
        "#\n",
        "# furthermore the signal can be observed better in a short\n",
        "# period of time\n",
        "duration = 0.5\n",
        "\n",
        "# this automatically defines out number of samples and\n",
        "# the generates a time array\n",
        "n_samp = int(Fs*duration)\n",
        "t = np.linspace(0,duration, n_samp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NFxO5fcCmUi"
      },
      "source": [
        "Now define the frequencies you want the signal to consist of."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExMftWPMCvPA"
      },
      "outputs": [],
      "source": [
        "freqs = [420, 500, 1300]\n",
        "\n",
        "# automatically generate the signal by addition of sines\n",
        "signal = np.sum([np.sin(2*np.pi*t*f) for f in freqs], axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPw2f200DWj8"
      },
      "source": [
        "Plotting the signal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "ya-jO9MTDWBz",
        "outputId": "050cd96c-3fc0-4ff9-c4d6-32b574a09b13"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[20,4])\n",
        "plot(t,signal)\n",
        "title('Original signal')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('X(t)')\n",
        "plt.xlim([0,duration])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RENalUoHDZ4W"
      },
      "outputs": [],
      "source": [
        "# if you want you can save the original signal as wav file and listen to it\n",
        "wavfile.write('original.wav',Fs, signal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Theory preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oU5r9Fz8D7jC"
      },
      "source": [
        "Now let's go a bit into theory. Normally you would sample the signal at the Nyquist frequency which is defined as $ f_{nyquist} = 2 * f_0 $ where $ f_0 $ is the bandwidth of your signal.\n",
        "Since human hearing is know to be in the range from $20$ Hz to $20$ kHz we only need to cover a bandwidth of $20$ kHz.\n",
        "Now multiply by $2$and we come to the $40$ kHz.\n",
        "So for lossless sampling of our defined signal in the hearing bandwith of humans,\n",
        "we need to sample it at 40 kHz which is taking a sample at each $ 1/40000 = 0.000025 $s.\n",
        "\n",
        "Now we learned in the chapter of compressive sensing, that it can be possible to randomly sample the signal with length p and only take m samples with $m < p$.\n",
        "The signal would then be represented by\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{bmatrix}\n",
        "\\\\ y \\\\ \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\ \\ \\ A \\ \\ \\ \n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\\\ x \\\\ \\\\\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "where y $\\in ℂ^{m}$, A $\\in ℂ^{mxp}$ and x $\\in ℂ^{p}$ [Eq. 10.1].\n",
        "One thing to overcome is to represent the signal in a basis which is s-spare to apply Gordon's theorem. For music signals this can be the Fourier-Base. For the demonstration we use the cosine tranformation which represents our signal as a sum of cosines where our signal is found to be s-sparse,\n",
        "\n",
        "\\begin{equation}\n",
        "X_k = \\sum_{n=0}^{N-1} x_n \\cos \\left[\\frac{\\pi}{N} \\left(n+\\frac{1}{2}\\right) k \\right] \\quad \\quad k = 0, \\dots, N-1.\n",
        "\\end{equation}\n",
        "\n",
        "So now let's transform the signal into the base where it is sparse.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz-q7LGIDwBz"
      },
      "outputs": [],
      "source": [
        "# define the width of our matrix A which is equals to the number\n",
        "# samples we want to take. Remeber that by Shannon this would\n",
        "# be 40'000 to cover human hearing!\n",
        "m = 1000\n",
        "\n",
        "# with this number we take m random sample points\n",
        "y = np.random.randint(0,n_samp,m)\n",
        "\n",
        "# if you want to see what happens when we just take the same number of samples \n",
        "# but with an equal distribution, uncomment this y and comment the other one\n",
        "# y = np.linspace(0,n_samp-1,m).astype(int)\n",
        "\n",
        "\n",
        "# since we must index by ascending values,\n",
        "# we have to sort the values\n",
        "y = np.sort(y)\n",
        "\n",
        "# now sample only m-times at random positions\n",
        "Y = signal[y]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRMWjArUMm-V"
      },
      "source": [
        "Let's have a look at the sample points. They are shown as red crosses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "tUqHkXaFQ3sM",
        "outputId": "aefee971-be0b-4dba-a448-caba48c3805e"
      },
      "outputs": [],
      "source": [
        "figure(figsize=[10,4])\n",
        "plot(t,signal,'b',t[y],Y,'rx')\n",
        "title('Original signal with random sample points')\n",
        "plt.xlabel('Time (s)')\n",
        "plt.ylabel('X(t)')\n",
        "plt.xlim([0,duration])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeW54WIcM8e_"
      },
      "source": [
        "The samples are not yet represented sparse. For this we have to define the base functions for the discrete cosine transform with the help of a library. Since the sample times are random, we have to build all the possible components and then chose the ones which correspond to our random samples. This method is also somewhat described in Chapter 10.3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34j_TjUYJ22f",
        "outputId": "6b31d228-5f79-4e8c-9f4a-bd378f821a70"
      },
      "outputs": [],
      "source": [
        "# construct the DCT basis functions\n",
        "# this can take quite a while depending on the\n",
        "# duration you chose\n",
        "DCT = dct(np.eye(n_samp))\n",
        "# take the same random samples from the base as from the signal\n",
        "A = DCT[y]\n",
        "print(\"The shape corresponds to m =\", A.shape[0], \" and p =\", A.shape[1])               "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HPSON1bOAVg"
      },
      "source": [
        "Now we need to solve the underteremined system knowing that our signal is sparse. It follows from Equation 10.2 that this approach corresponds to solving\n",
        "$$ min || z||_0 \\text{ such that } Az=y.$$\n",
        "As written in the lecture notes, this problem is in general NP-hard and normally the following optimization problem is considered, since the $l_1$ unit ball becomes very \"pointy\" in high dimensions, which reminds of the $l_0$ norm:\n",
        "$$ min || z||_1 \\text{ such that } Az=y.$$\n",
        "\n",
        "In reality, the solution is very often solved via Lasso Regression, which focuses on a $l_1$ constraint. Read more about it on [This Paper](https://www.stat.cmu.edu/~ryantibs/statml/lectures/sparsity.pdf). The derivation of the dual formulation can be found in Chapture 10.3 of the lecture notes. By sacrificing the exact recovery and by considering a signal with noise $w$ one can write the measurement as\n",
        "$$ y=Ax+w.$$\n",
        "From this we can arrive to the practical formulation\n",
        "$$ \\min_{x} \\frac{1}{2}||Ax - y||_2^2 + λ||x||_1$$\n",
        "which corresponds to the Lasso Regression.\n",
        "\n",
        "Note the two conditions written in the lecture note. The first one is already satisfied, as we already randomly selected the support. The non-zero entries of sign(x) seem to form a Steinhaus sequence by watching the distribution of the values, but I didn't try to prove it, it seems a bit strange since the signal is very deterministic. Maybe someone can try to elaborate this?\n",
        "\n",
        "Anyways, back to the problem. The matrix $A$ is already given in reduced form where we know that x is sparse, as well as the signal which is denoted as $y$.\n",
        "The vector $x$ corresponds to the sparse solution. We can use libraries like scikit-learn to find the corresponding solution.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "t06klu9sUTrT",
        "outputId": "a165a99d-4cc7-4c98-d99d-3660e4787fde"
      },
      "outputs": [],
      "source": [
        "# the Lasso model solves the equation just described\n",
        "# minimization problem for us.\n",
        "# In sklearn the solution x is saved in the lasso coefficients\n",
        "# the parameter alpha corresponds to lambda, which weights our l1 solution\n",
        "# it is a hyperparameter which normally must be optimized for a given problem\n",
        "from sklearn.linear_model import Lasso \n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(A,Y)\n",
        "x_sparse = lasso.coef_\n",
        "plot(x_sparse)\n",
        "title(\"Spare signal representation\")\n",
        "plt.ylabel(\"Coefficient\")\n",
        "plt.xlabel(\"Index\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdpC4N0BqDo_"
      },
      "source": [
        "Let's look at the sparsness of the vector and find the s value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF7AOSFGpnOO",
        "outputId": "f6be7538-acd3-44d2-c326-2d0df882532d"
      },
      "outputs": [],
      "source": [
        "sparseness = np.sum(x_sparse== 0)/n_samp\n",
        "print(\"Solution is {} % sparse\".format(100.*sparseness))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4GN6l1NqRAz"
      },
      "source": [
        "As you see this is a very sparse vector! We can directly print the nonzero values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsksOekKYR7_",
        "outputId": "8575c9b1-e6f8-438f-9573-4de80a29e907"
      },
      "outputs": [],
      "source": [
        "coefs = x_sparse[x_sparse != 0]\n",
        "print(\"There are \", len(coefs), \"nonzero entries in the vector. The vector looks as follows:\")\n",
        "print(coefs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6AZSqEJq0DH"
      },
      "source": [
        "Instead of taking 40000 samples we now have only around 30 (may change on the random choice) nonzero values after taking only m instead of p samples. For storage optimisation the coeficients could be saved with informationa about their index and the cosine base. Even with the additional information this is much less than saving 40000 values. But so how good can this be recovered? For recovery we do an inverse cosine transform on the sparse vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nukk2MKqVDab"
      },
      "outputs": [],
      "source": [
        "Xhat = idct(x_sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLaTYXj0rtj-"
      },
      "source": [
        "Let's plot the reconstructed signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "Gdj-7zY1VGCQ",
        "outputId": "f6a10814-ed26-4224-eee0-9b5c5d16ee23"
      },
      "outputs": [],
      "source": [
        "figure()\n",
        "plot(t,Xhat)\n",
        "title('Reconstructed signal')\n",
        "figure()\n",
        "plot(t,Xhat-signal)\n",
        "title('Differense of the original and the reconstruction')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P53xwasQVUcs"
      },
      "outputs": [],
      "source": [
        "# If you want, save the reconstructed sound and listen to it.\n",
        "wavfile.write('sound_reconstructed.wav',Fs, Xhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFQBdzwXsG3V"
      },
      "source": [
        "So as we see, the reconstruction is not perfect. Why is that?\n",
        "At first let's qickly remember Gordon's theorem and that it suggests \n",
        "$$m≈s*log(\\frac{s}{p})$$\n",
        "being enough to recover x. You can quickly grab a calculator and compare the left and right side of the equation. Given that you didn't change the values the m is much larger than needed. But we aren't really solving the original optimization problem, but a relaxed one which is basically a least square regression with $l_1$ penalty and which even has a regularization parameter $\\lambda$ to choose. Read more about it in the last chapters of the lecture note. So it is clear that we dind't really optimise the original problem. One good thing about sound is, that it's observation is quite subjective. Do you personally hear a difference? You can try changing the m and render different versions of the generated signals. You will observe that for some bigger m, which is still a lot smaller than p, one cannot hear a difference anymore.\n",
        "\n",
        "Another problem is the discrete cosine transformation which can introduce aliasing and roundoff errors due to fixed point arithmetics. This actually is a much higher factor than m, since it appears that this fading stays even with very high m. It seems like this is a common problem of the Discrete Cosine Transform and is due to the fixed size windows which are used. Similar problems appear in DTFT. This is why modern audio compression techniques use [Modified discrete cosine transform](https://en.wikipedia.org/wiki/Modified_discrete_cosine_transform). So actually we can focus on just the inner part of the reconstructed signal and appreciate how well it worked!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison to non-random sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To show how the results change, the experiment can be repeated with taking samples which are linearly spaced in time and not random. Go to the first\n",
        "code block in the demo, comment the random sample points and uncomment the prepared linear sampling."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Compressive_Sensing_Audio.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a578b60687b99a82ca78389cf5818de06cdae5f0e0fb7a238655a79bce45af2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
